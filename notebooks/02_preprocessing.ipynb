{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41020117",
   "metadata": {},
   "source": [
    "## Libraries and settings\n",
    "\n",
    "Import the required libraries and set the main settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cc3fb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/data_analytics_project/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0887c0d8",
   "metadata": {},
   "source": [
    "# Weather & Bicycle Usage – Data Preprocessing\n",
    "\n",
    "This notebook cleans, merges, and prepares the collected weather and bicycle traffic data for Zurich for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1765b8",
   "metadata": {},
   "source": [
    "# Weather & Bicycle Usage – Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d3bb6",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ccc484c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/data_analytics_project/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8b2832",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d9a1ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather data shape: (361, 2)\n",
      "         date  temp_mean\n",
      "0  2025-01-01        0.6\n",
      "1  2025-01-02        3.1\n",
      "2  2025-01-03       -0.3\n",
      "3  2025-01-04       -1.9\n",
      "4  2025-01-05        1.8\n",
      "\n",
      "Data types:\n",
      "date          object\n",
      "temp_mean    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load daily mean temperature data (English, mean only)\n",
    "weather_df = pd.read_csv(\"../data/temperature_mean_zurich_2025.csv\")\n",
    "print(\"Weather data shape:\", weather_df.shape)\n",
    "print(weather_df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(weather_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae54098",
   "metadata": {},
   "source": [
    "## Loading datasets\n",
    "\n",
    "We load both the weather and bicycle counter data from the CSV files generated in the data collection phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29d56af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bicycle data shape: (838029, 8)\n",
      "   FK_STANDORT             DATUM  VELO_IN  VELO_OUT  FUSS_IN  FUSS_OUT  \\\n",
      "0         4241  2025-01-01T00:00      3.0       0.0      NaN       NaN   \n",
      "1         2989  2025-01-01T00:00      1.0       2.0      NaN       NaN   \n",
      "2         2991  2025-01-01T00:00     18.0       2.0      NaN       NaN   \n",
      "3         4255  2025-01-01T00:00      0.0       0.0      NaN       NaN   \n",
      "4         4242  2025-01-01T00:00      1.0       0.0      NaN       NaN   \n",
      "\n",
      "       OST     NORD  \n",
      "0  2682297  1248328  \n",
      "1  2682278  1248324  \n",
      "2  2682756  1247323  \n",
      "3  2682881  1246549  \n",
      "4  2682337  1248451  \n",
      "\n",
      "Data types:\n",
      "FK_STANDORT      int64\n",
      "DATUM           object\n",
      "VELO_IN        float64\n",
      "VELO_OUT       float64\n",
      "FUSS_IN        float64\n",
      "FUSS_OUT       float64\n",
      "OST              int64\n",
      "NORD             int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load bicycle counter data\n",
    "bikes_df = pd.read_csv(\"../data/2025_verkehrszaehlungen_werte_fussgaenger_velo.csv\")\n",
    "print(\"Bicycle data shape:\", bikes_df.shape)\n",
    "print(bikes_df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(bikes_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbdaeff",
   "metadata": {},
   "source": [
    "## Cleaning and handling missing values\n",
    "\n",
    "We convert time columns to datetime format and remove any rows with missing values to ensure data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a39988ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After datetime conversion:\n",
      "Weather data types: [dtype('<M8[ns]') dtype('float64')]\n",
      "Bikes data types: [dtype('int64') dtype('<M8[ns]') dtype('float64') dtype('float64')\n",
      " dtype('float64') dtype('float64') dtype('int64') dtype('int64')]\n"
     ]
    }
   ],
   "source": [
    "# Convert date columns to datetime\n",
    "weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"])\n",
    "bikes_df[\"DATUM\"] = pd.to_datetime(bikes_df[\"DATUM\"])\n",
    "print(\"After datetime conversion:\")\n",
    "print(\"Weather data types:\", weather_df.dtypes.values)\n",
    "print(\"Bikes data types:\", bikes_df.dtypes.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "261fb0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated daily bicycle data:\n",
      "        date  VELO_IN  VELO_OUT  FUSS_IN  FUSS_OUT\n",
      "0 2025-01-01   5094.0    2558.0   1188.0    1099.0\n",
      "1 2025-01-02   5086.0    2423.0    541.0     448.0\n",
      "2 2025-01-03   9073.0    4420.0    450.0     404.0\n",
      "3 2025-01-04   7129.0    3551.0    457.0     388.0\n",
      "4 2025-01-05   5000.0    2641.0    630.0     565.0\n"
     ]
    }
   ],
   "source": [
    "# --- Aggregate bicycle data to daily level ---\n",
    "# Create a new column with only the date (no time)\n",
    "bikes_df['DATE_ONLY'] = bikes_df['DATUM'].dt.date\n",
    "\n",
    "# Sum all bike counts per day (across all locations)\n",
    "daily_bikes = bikes_df.groupby('DATE_ONLY').agg({\n",
    "    'VELO_IN': 'sum',\n",
    "    'VELO_OUT': 'sum',\n",
    "    'FUSS_IN': 'sum',\n",
    "    'FUSS_OUT': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "daily_bikes.rename(columns={'DATE_ONLY': 'date'}, inplace=True)\n",
    "# Convert 'date' back to datetime for merging\n",
    "daily_bikes['date'] = pd.to_datetime(daily_bikes['date'])\n",
    "\n",
    "print('Aggregated daily bicycle data:')\n",
    "print(daily_bikes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8daae27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in weather data:\n",
      "date         0\n",
      "temp_mean    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in bicycle data:\n",
      "FK_STANDORT         0\n",
      "DATUM               0\n",
      "VELO_IN         76402\n",
      "VELO_OUT       167593\n",
      "FUSS_IN        761627\n",
      "FUSS_OUT       761627\n",
      "OST                 0\n",
      "NORD                0\n",
      "DATE_ONLY           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in weather data:\")\n",
    "print(weather_df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in bicycle data:\")\n",
    "print(bikes_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7fd58c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping missing values:\n",
      "Weather data shape: (361, 2)\n",
      "Bicycle data shape: (0, 9)\n"
     ]
    }
   ],
   "source": [
    "# Drop missing values\n",
    "weather_df = weather_df.dropna()\n",
    "bikes_df = bikes_df.dropna()\n",
    "\n",
    "print(\"After dropping missing values:\")\n",
    "print(\"Weather data shape:\", weather_df.shape)\n",
    "print(\"Bicycle data shape:\", bikes_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0d4b182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in weather data: 0\n",
      "Duplicates in bicycle data: 0\n",
      "\n",
      "After removing duplicates:\n",
      "Weather data shape: (361, 2)\n",
      "Bicycle data shape: (0, 9)\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "print(\"Duplicates in weather data:\", weather_df.duplicated().sum())\n",
    "print(\"Duplicates in bicycle data:\", bikes_df.duplicated().sum())\n",
    "\n",
    "# Drop duplicates if any\n",
    "weather_df = weather_df.drop_duplicates()\n",
    "bikes_df = bikes_df.drop_duplicates()\n",
    "\n",
    "print(\"\\nAfter removing duplicates:\")\n",
    "print(\"Weather data shape:\", weather_df.shape)\n",
    "print(\"Bicycle data shape:\", bikes_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb67444",
   "metadata": {},
   "source": [
    "## Merging weather and bicycle datasets\n",
    "\n",
    "We perform an inner join on the time column to combine both datasets into a single analysis-ready dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c984edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data shape: (359, 6)\n",
      "\n",
      "First 5 rows:\n",
      "        date  temp_mean  VELO_IN  VELO_OUT  FUSS_IN  FUSS_OUT\n",
      "0 2025-01-01        0.6   5094.0    2558.0   1188.0    1099.0\n",
      "1 2025-01-02        3.1   5086.0    2423.0    541.0     448.0\n",
      "2 2025-01-03       -0.3   9073.0    4420.0    450.0     404.0\n",
      "3 2025-01-04       -1.9   7129.0    3551.0    457.0     388.0\n",
      "4 2025-01-05        1.8   5000.0    2641.0    630.0     565.0\n",
      "\n",
      "Data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 359 entries, 0 to 358\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   date       359 non-null    datetime64[ns]\n",
      " 1   temp_mean  359 non-null    float64       \n",
      " 2   VELO_IN    359 non-null    float64       \n",
      " 3   VELO_OUT   359 non-null    float64       \n",
      " 4   FUSS_IN    359 non-null    float64       \n",
      " 5   FUSS_OUT   359 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(5)\n",
      "memory usage: 17.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Merge weather and aggregated bicycle data on date (daily mean only)\n",
    "merged_df = pd.merge(weather_df, daily_bikes, on=\"date\", how=\"inner\")\n",
    "\n",
    "print(\"Merged data shape:\", merged_df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(merged_df.head())\n",
    "print(\"\\nData info:\")\n",
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ba0f463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic statistics:\n",
      "                      date   temp_mean       VELO_IN      VELO_OUT  \\\n",
      "count                  359  359.000000    359.000000    359.000000   \n",
      "mean   2025-06-29 00:00:00   10.576323  23350.487465  11717.409471   \n",
      "min    2025-01-01 00:00:00   -2.900000   1373.000000   1241.000000   \n",
      "25%    2025-03-31 12:00:00    4.500000  17209.500000   8603.000000   \n",
      "50%    2025-06-29 00:00:00   10.000000  22996.000000  11533.000000   \n",
      "75%    2025-09-26 12:00:00   16.450000  28954.000000  14785.000000   \n",
      "max    2025-12-25 00:00:00   26.800000  47470.000000  23479.000000   \n",
      "std                    NaN    7.308193   9144.180319   4650.774839   \n",
      "\n",
      "           FUSS_IN     FUSS_OUT  \n",
      "count   359.000000   359.000000  \n",
      "mean   1220.885794   941.256267  \n",
      "min       0.000000     0.000000  \n",
      "25%     622.500000   492.000000  \n",
      "50%    1000.000000   837.000000  \n",
      "75%    1643.000000  1286.500000  \n",
      "max    4116.000000  3340.000000  \n",
      "std     795.581763   558.080430  \n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "print(\"Basic statistics:\")\n",
    "print(merged_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18c635dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to ../data/merged_weather_bikes.csv\n"
     ]
    }
   ],
   "source": [
    "# Save merged data\n",
    "merged_df.to_csv(\"../data/merged_weather_bikes.csv\", index=False)\n",
    "print(\"Merged data saved to ../data/merged_weather_bikes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca8c5eb",
   "metadata": {},
   "source": [
    "## Saving cleaned dataset\n",
    "\n",
    "The merged and cleaned dataset is saved as a CSV file for use in exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c759936",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this notebook, we successfully cleaned and merged the daily mean temperature and bicycle counter datasets for Zurich. Missing values were removed and duplicates were eliminated. The final merged dataset contains daily observations with mean temperature and bicycle counts. The cleaned dataset is now ready for exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73167592",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5d877ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "POSIX\n",
      "Linux | 6.8.0-1030-azure\n",
      "Datetime: 2025-12-28 19:00:15\n",
      "Python Version: 3.12.3\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
